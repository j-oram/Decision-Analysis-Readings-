<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Jacob Oram" />

<meta name="date" content="2022-12-15" />

<title>Chapter 4: Bayesian Analysis</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Bayesian Decision Analysis Readings</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Intro</a>
</li>
<li>
  <a href="Berger-Ch2.html">Berger Ch.2</a>
</li>
<li>
  <a href="Berger-Ch3.html">Berger Ch.3</a>
</li>
<li>
  <a href="Berger-Ch4.html">Berger Ch.4</a>
</li>
<li>
  <a href="Other.html">Other</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Chapter 4: Bayesian Analysis</h1>
<h4 class="author">Jacob Oram</h4>
<h4 class="date">2022-12-15</h4>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This is a massive chapter, because it covers the entire Bayesian
paradigm. Perhaps because this book was written in 1985, the first
section of this chapter begins with seven detailed arguments in favor of
using Bayesian analysis (perhaps useful for a future Stat 532 course
debate). The second section introduces the concept of a posterior
distribution. The third section is about Bayesian inference. I focused
my reading on section four, which is concerned with Bayesian decision
theory.</p>
</div>
<div id="definitions" class="section level2">
<h2>Definitions</h2>
<hr>
<p><strong>Definition 8a</strong>: The <em>posterior expected loss</em>
of an action <span class="math inline">\(a\)</span>, is</p>
<p><span class="math display">\[
\rho(\pi(\theta|x), a) = \int_\Theta L(\theta, a) \pi(\theta|x)d\theta,
\]</span></p>
<p>where <span class="math inline">\(\pi(\theta|x)\)</span> is the
posterior distribution on <span
class="math inline">\(\theta.\)</span></p>
<p>Essentially, we are averaging the loss of <span
class="math inline">\(a\)</span> over all the possible states of nature
<span class="math inline">\(\theta.\)</span> With this updated
definition (see Chapter 1), we can also define the <em>posterior Bayes
action</em></p>
<strong>Definition 8b</strong>: The <em>posterior Bayes action</em>,
<span class="math inline">\(\delta^\pi(x)\)</span> is any action <span
class="math inline">\(a \in \mathscr{A}\)</span> which minimizes the
posterior expected loss, <span
class="math inline">\(\rho(\pi(\theta|x),a).\)</span>
<hr>
<hr>
<p><strong>Definition 6</strong>: (Chapter 1) The <em>Bayes risk</em> of
a decision rule <span class="math inline">\(\delta,\)</span> with
respect to posterior distribution <span
class="math inline">\(\pi\)</span> on <span
class="math inline">\(\Theta\)</span> is defined as <span
class="math display">\[
r(\pi, \delta) = E^\pi[R(\theta, \delta)].
\]</span></p>
<p>The decision rule <span class="math inline">\(\delta^\pi\)</span>
which minimizes <span class="math inline">\(r(\pi, \delta)\)</span> is
called a <em>Bayes Rule</em> and the quantity <span
class="math inline">\(r(\pi) = r(\pi, \delta^\pi)\)</span> is called the
<em>Bayes risk</em> for <span class="math inline">\(\pi\)</span>.</p>
<p>Note to self: the Bayes risk is obtained by first averaging the loss
over the data X (to obtain the risk), then averaging the risk over the
distribution for <span class="math inline">\(\theta:\)</span></p>
<p><span class="math display">\[
r(\pi, \delta) = \int_\Theta R(\theta, \delta)\pi(\theta)d\theta \\
= \int_\Theta \left[ \int_\mathscr{X} L(\theta, \delta(x)) f(x|\theta)dx
\right] \pi(\theta) d\theta
\]</span></p>
<hr>
<strong>Definition 9</strong>: If <span
class="math inline">\(\pi\)</span> is an improper prior, but <span
class="math inline">\(\delta^\pi(x)\)</span> is an action which
minimizes the expected posteriior loss <span
class="math inline">\(\rho(\pi(\theta|x), \delta(x))\)</span> for each
<span class="math inline">\(x\)</span> with <span
class="math inline">\(m(x)&gt;0\)</span>, then <span
class="math inline">\(\delta^\pi\)</span> is called a <em>generalized
Bayes rule</em>.
<hr>
<p><strong>Definition 10</strong>: The <em><span
class="math inline">\(\Gamma\)</span>-posterior expected loss of <span
class="math inline">\(a_0\)</span></em> is</p>
<p><span class="math display">\[
\rho_\Gamma(a_0) = \sup_{\pi \in \Gamma} \rho(\pi(\theta|x), a_0),
\]</span></p>
where <span class="math inline">\(\Gamma\)</span> is a class of possible
prior distributions on <span class="math inline">\(\theta\)</span>.
<hr>
<p><strong>Definition 11</strong>: An action <span
class="math inline">\(a_0\)</span> is said to be <em><span
class="math inline">\(\epsilon\)</span>-posterior robust</em> with
respect to prior class <span class="math inline">\(\Gamma\)</span> if ,
for all <span class="math inline">\(\pi \in \Gamma\)</span>, <span
class="math display">\[
\left| \rho(\pi(\theta|x), a_0) - \inf_a \rho(\pi(\theta|x), a)\right|
\leq \epsilon
\]</span></p>
</div>
<div id="key-results" class="section level2">
<h2>Key Results</h2>
<p><strong>Result 2</strong> If <span
class="math inline">\(\delta\)</span> is a nonrandomized estimator, then
<span class="math display">\[ r(\pi, \delta) = \int_{\{x:m(x)&gt;0\}}
\rho(\pi(\theta|x),\delta(x))m(x)dx\]</span></p>
<p><strong>Result 1</strong> A Bayes rule <span
class="math inline">\(\delta^\pi\)</span> (i.e., a rule minimizing <span
class="math inline">\(r(\pi, \delta))\)</span> can be found by choosing,
for each <span class="math inline">\(x\)</span> such that <span
class="math inline">\(m(x) &gt; 0\)</span>, an action which minimizes
the posterior expected loss. The rule can be defined arbitrarily when
<span class="math inline">\(m(x) = 0\)</span>.</p>
<p>The results are listed out of order because 1 follows from 2.
Together they imply that minimizing the (frequentist) Bayes risk <span
class="math inline">\(r(\pi, \delta)\)</span> is essentially the same
problem as minimizing the expected posterior loss.</p>
</div>
<div id="section-4.4-bayesian-decision-theory" class="section level2">
<h2>Section 4.4: Bayesian Decision Theory</h2>
<p>After taking the reader through the above results and definitions,
Berger outlines the Bayes rules for estimation with several standard
loss functions. In estimation, the Bayes rule is the estimator denoted
as <span class="math inline">\(a\)</span> or <span
class="math inline">\(\delta^\pi\)</span> that minimizes posterior
expected loss. I did not write up the results here because they would
are examples more than key theoretical results.</p>
<p>The next subsubsection involves finite action problems; these include
hypothesis testing and (more interestingly for modern problems)
classification. The use of decision analysis in classification is
illustrated with an example (p.166).</p>
</div>
<div id="section-4.5-empirical-bayes" class="section level2">
<h2>Section 4.5: Empirical Bayes</h2>
<p>Briefly, empirical Bayes problems are ones where known relationships
between the parameter values allow the use of the data to inform the
prior distribution. An example is in a multi-level model, where
parameters <span class="math inline">\(\theta_i\)</span> arise from a
common population with its own distribution controlled by
hyperparameters. The trick is that the population model is then
interpreted as the prior.</p>
<p>An example is given in subsection 4.5.2, which focuses on parametric
methods for empirical Bayes:</p>
<p>Suppose that <span class="math inline">\(X_i\)</span> are realized
test scores of students. The assumed likelihood for these data is</p>
<p><span class="math display">\[
X_i \sim N(\theta_i, \sigma^2_f) \\
\theta_i \sim N(\mu_\pi, \sigma^2_\pi),
\]</span></p>
<p>where <span class="math inline">\(\theta_i\)</span> is assumed to be
the true ability level of individual <span
class="math inline">\(i\)</span> and <span
class="math inline">\(\sigma_f^2\)</span> is (known) reliability of test
scores. We assume that these true ability levels are normally
distributed in the population, with mean <span
class="math inline">\(\mu_pi\)</span> and variance <span
class="math inline">\(\sigma^2_\pi\)</span> (both of which are unknown).
The empirical Bayes approach is concerned with estimating <span
class="math inline">\(\mu_\pi\)</span> and <span
class="math inline">\(\sigma^2_\pi\)</span> from the data using the
marginal distribution <span class="math inline">\(m(x)\)</span>. This
follows somewhat closely with the hierarchical model laid out in Gelman
et al., (2013), section 5.4 and the eight schools example in section
5.5.</p>
</div>
<div id="hierarchical-bayes" class="section level2">
<h2>4.6 Hierarchical Bayes</h2>
<p>I skipped this section because I prefer the treatment in Gelman et
al., (2013).</p>
</div>
<div id="bayesian-robustness" class="section level2">
<h2>4.7 Bayesian Robustness</h2>
<p>This section addresses robustness of Bayesian analysis to prior
specification. There is a very short discussion of robustness of
decision analysis to the choice of loss function at the very end of the
section, but the author argues that the choice of prior is far more
challenging with greater implications for resulting decisions.</p>
<p>More interesting is how posterior robustness is framed in terms of
actions <span class="math inline">\(a\)</span>. There seems to be an
implicit assumption that the action <span
class="math inline">\(a\)</span> is a choice of estimate for the
parameter of interest, but I see no reason why we would not be able to
extend this to possible actions after inference has been made. The idea
of posterior expected loss for a class of priors (as given in Definition
10 above) can be extended to consider a range of values for the
posterior expected losses for action <span
class="math inline">\(a_0\)</span>:</p>
<p><span class="math display">\[
\left(
\inf_{\pi \in \Gamma} \rho(\pi(\theta|x), a_0), \sup_{\pi \in \Gamma}
\rho(\pi(\theta|x), a_0)
\right).
\]</span></p>
</div>
<div id="admissability-of-bayes-rules" class="section level2">
<h2>4.8 Admissability of Bayes Rules</h2>
<p>Generally, a Bayes rule is always admissible (i.e., there does not
exist a different rule <span class="math inline">\(\delta\)</span> with
lower risk for all <span class="math inline">\(\theta \in
\Theta\)</span>), because if there was, that alternative rule would also
have lower Bayes risk <span class="math inline">\(r(\pi,
\delta).\)</span> The following three theorems are proven by
contradiction using this line of reasoning.</p>
<p><strong>Theorem 7 (9)</strong>: Assume that <span
class="math inline">\(\Theta\)</span> is discrete (say <span
class="math inline">\(\Theta = \{\theta_1, \theta_2, \dots\}\)</span>)
and that the prior <span class="math inline">\(\pi\)</span> gives
positive mass to each <span class="math inline">\(\theta_i \in
\Theta\)</span>. A Bayes rule <span
class="math inline">\(\delta^\pi\)</span> is then admissible with
respect to <span class="math inline">\(\pi\)</span>.</p>
<p>The statement for theorem 9 is essentially the same, but for a
continuous parameter space. In this case, the theorem assumes that the
risk function is continuous and that the prior gives positive
probability to any open subset of <span
class="math inline">\(\Theta.\)</span></p>
<p><strong>Theorem 8</strong>: If a Bayes rule is unique, then it is
admissible.</p>
<p>Berger also discusses generalized Bayes rules, which are Bayes rules
from an improper prior. He mentions that although it’s less clear what
we mean exactly by <span class="math inline">\(r(\pi, \delta)\)</span>
if the prior is improper, the Bayes rule minimizing <span
class="math inline">\(r(\pi, \delta)\)</span> is still found by
minimizing posterior expected loss. When the expected risk, averaging
over prior uncertainty in <span class="math inline">\(\theta\)</span> is
not finite (that is, when <span class="math inline">\(r(\pi, \delta) =
\infty\)</span>, which is not uncommon when the prior is improper), even
natural Bayes rules (such as the posterior mean in estimation problems)
may not be admissible.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
