<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Jacob Oram" />

<meta name="date" content="2022-12-15" />

<title>Other readings</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Bayesian Decision Analysis Readings</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Intro</a>
</li>
<li>
  <a href="Berger-Ch1.html">Berger Ch.1</a>
</li>
<li>
  <a href="Berger-Ch2.html">Berger Ch.2</a>
</li>
<li>
  <a href="Berger-Ch3.html">Berger Ch.3</a>
</li>
<li>
  <a href="Berger-Ch4.html">Berger Ch.4</a>
</li>
<li>
  <a href="Other.html">Other</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Other readings</h1>
<h4 class="author">Jacob Oram</h4>
<h4 class="date">2022-12-15</h4>

</div>


<p>This chapter covers other readings, including papers that have
applied decision analysis in ecology and conservation biology and the
ninth chapter of <em>Bayesian Data Analysis</em> <span
class="citation">(Gelman 2013)</span>.</p>
<div id="wade00-bayesian-methods-in-conservation-biology"
class="section level2">
<h2><span class="citation">(Wade 2000)</span> Bayesian Methods in
Conservation Biology</h2>
<p>The main thrust of this article is that practitioners in conservation
biology may find Bayesian methods far more appropriate for biological
data, easier to handle and interpret when presenting results, with prior
information explicitly incorporated to improve scientific transparency.
The author first provides a simple regression example to demonstrate a
situation in which Bayesian methods yield far more reasonable results
(in the sense that it “matches what most people would conclude without
using statistics”). The regression example is then included in a formal
decision analysis, developing the loss functions directly (as opposed to
defining a utility function and then defining <span
class="math inline">\(L = -U(\theta, a)\)</span>, which is the approach
that Berger advocates for). Although this is a very simple example, it
provides an excellent high-level overview of Bayesian analysis and
decision theory, with a clear argument that these methods will
ultimately be easier to understand and communicate when conducting
conservation-oriented studies in biology and ecology.</p>
</div>
<div
id="williams16-combining-statistical-inference-and-decisions-in-ecology"
class="section level2">
<h2><span class="citation">(Williams and Hooten 2016)</span> Combining
statistical inference and decisions in ecology</h2>
<p>The authors introduce statistical decision theory (SDT) for
ecologists, noting that it has been widely applied in operations
research as well as in statistics and mathematics. [In statistics, a
decision rule could be a choice of estimator, such as the posterior
mean; <span class="citation">Berger (1985)</span>]. Noting that
statistics originated as a sub-branch of decision theory, and that the
reemergence of the Bayesian paradigm is partly due to its natural
compatability with decision theory, the authors summarize the highlights
of statistical decision theory that are revelant for the field of
ecology – essentially, this is briefly restating the first four chapters
of <span class="citation">Berger (1985)</span>. Following this overview,
the <span class="citation">Williams and Hooten (2016)</span> address two
classes of problems: choosing an estimator to be reported (“optimal
point estimation”) and choosing a management strategy for a natural
resource (“optimal resource management”). For the problem of optimal
point estimation, the authors note that the correctly specifying the
loss function is critical for appropriate inference.</p>
<p>The authors illustrate optimal resource management through an example
wherein land managers must decide the burn interval to optimize
cumulative abundance of Henslow’s Sparrows at Big Oaks National Wildlife
Refuge in southeastern Indiana.<br />
This example provides a beautiful example of constructing a loss
function from first principles (e.g., the statement that more frequent
burn intervals are more costly and should have higher loss). The model
used in the case study is</p>
<p><span class="math display">\[
y_{j,t} \sim Poisson(A_j\lambda_{j,t}) \\
log(\lambda_{j,t}) = x_{j,t}\boldsymbol{\beta} + \eta_j \\
\]</span></p>
<p>where <span class="math inline">\(y_{j,t}\)</span> is the count of
sparrows at site <span class="math inline">\(j\)</span> during time
<span class="math inline">\(t\)</span>, <span
class="math inline">\(A_j\)</span> is the area of site <span
class="math inline">\(j = 1, 2, \dots, 8\)</span>, <span
class="math inline">\(\lambda_{j,t}\)</span> is the unknown density of
Henslows sparrows at site <span class="math inline">\(j\)</span> during
time <span class="math inline">\(t\)</span>, and is a function of
regression coefficients <span class="math inline">\(\boldsymbol{\theta}
= (\boldsymbol{\beta \eta})^T\)</span>. The covariate <span
class="math inline">\(x\)</span> represents the number of summers since
the last burn. The random effect <span
class="math inline">\(\eta_j\)</span> is a random effect that adjusts
the density of Henslows sparrows at each site. The priors for this model
were</p>
<p><span class="math display">\[
\boldsymbol{\beta} \sim Normal(\boldsymbol{\mu}, \sigma^2I) \\
\eta_j \sim Normal(0,1)
\]</span></p>
<p>with <span class="math inline">\(\mu^T = (-5, 2.5,0.2, 0.2)\)</span>
and <span class="math inline">\(\sigma^2 = 10\)</span> being informed by
a prior study. The abundance over which posterior uncertainty is
estimated is a derived quantity resulting from <span
class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>The authors’ case study utilized a standard loss function (linear
loss), which is in keeping with their advice to practitioners. They
acknowledge that choosing a loss function is just as subjective as
choosing a likelihood and a prior, but the body of literature is many
times larger for the latter two. In cases where information about
potential loss is lacking, the authors suggest that standard loss
functions are a good starting point. They argue that the most important
aspects to consider when constructing a loss function are the shape
(e.g., convex or concave) and the symmetry.</p>
</div>
<div
id="dorazio03-bayesian-inference-and-decision-theory-a-framework-for-decision-making-in-natural-resource-management"
class="section level2">
<h2><span class="citation">(Dorazio and Johnson 2003)</span> Bayesian
inference and decision theory: A framework for decision making in
natural resource management</h2>
<p>The authors describe the link between Bayesian methods, decision
analysis and natural resource management. The introduction, which does
not include the in-depth description of Bayesian decision theoretical
machinery <span class="citation">(as in Williams and Hooten
2016)</span>, makes the argument essentially for the adaptation of
Bayesian methods (in general; this article is old). However, the main
thrust of the article appears very similar to <span
class="citation">Williams and Hooten (2016)</span>; a key difference is
that the authors provide an extended example wherein the model is
autoregressive and parameter estimates are repeatedly updated over
time.<br />
The authors discuss not only finding the expected loss for a single
future time (based on the posterior predictive distribution), but also
finding a sequence of optimal actions for future time steps.</p>
<p>Notes:</p>
<ul>
<li>This paper directly addresses decision making for adaptive
monitoring in the sense that it focuses on an example where the same
decision is repeatedly made at regular intervals. Through the sequential
updating of parameter estimates over time, managers learn over time what
determines optimal actions.<br />
</li>
<li>The authors assume that the optimal habitat state for the
hypothetical migratory birds in their example is known with
certainty.</li>
<li>The authors do not consider cost of management actions in their loss
function.</li>
</ul>
</div>
<div
id="varis99-learning-bayesian-decision-analysis-by-doing-lessons-from-environmental-and-natural-resources-management"
class="section level2">
<h2><span class="citation">(Varis and Kuikka 1999)</span> Learning
Bayesian decision analysis by doing: lessons from environmental and
natural resources management</h2>
<p>The authors highlight several case studies to summarize their
learning from applying Bayesian decision analysis. The decision analytic
frameworks they use in these case studies are based on influence
diagrams and belief networks, as summarized in <span
class="citation">Varis (1997)</span> (this work was originally developed
by Judea Pearl and others in the late 1980’s). Among the lessons learned
(there are many, see the discussion section), some key highlights
are:</p>
<ul>
<li>each modification of the model structure redefines the problem</li>
<li>structural uncertainty of a deterministic model can be modeled</li>
<li>Bayesian decision analysis is not just parameter estimation</li>
</ul>
<p>Some of these highlights are fairly apparent or apply outside of the
BDA context. The last point seems obvious but it is interesting because
it is related to a comment in a previous paper <span
class="citation">(Varis 1997)</span>: when the authors equate Bayesian
decision analysis with network based methods “the entire model - <em>the
hypothesis space</em> - is subjected to Bayesian analysis, not only the
parameter space”. This statement makes sense when we consider that
according to <span class="citation">Berger (1985)</span>, in a
hypothesis test setting the reject/FTR binary is itself a set of actions
that are evaluated based on expected posterior loss. However, in the
formulation of <span class="citation">Berger (1985)</span>, the entire
model is not subjected to Bayesian analysis – we still assume a
likelihood function, so there is some structural input from the
statistician about how the data arise. This appears to be a discrepancy
between how BDA is described by <span class="citation">Varis and Kuikka
(1999)</span> and <span class="citation">Berger (1985)</span>.</p>
<p>Another source of discrepancy (or perhaps a lack of understanding on
my part) arises from trying to connect networks to posterior loss. While
the components of the network models match the types of situations
described by <span class="citation">Berger (1985)</span> (e.g., an
objective function, a decision to be made, an unknown state of nature
with a probability distribution assigned to possible values), I haven’t
formally made the connection yet with this formulation of a decision
problem with minimizing posterior loss. One point of confusion for me is
the arc from the random “prior” node to the final satisfaction node (see
Figure @ref(fig:netex)). I read this arc as a statement: “if the state
of nature truly is <span class="math inline">\(\theta^\star\)</span>,
then my satisfaction (i.e., expected loss, utility), after gathering
data from screening and making a decision will be <span
class="math inline">\(X\)</span>.” However, in the formulation of <span
class="citation">Berger (1985)</span>, loss associated with an action is
influenced by the true value of <span
class="math inline">\(\theta\)</span> only through the posterior
distribution, which would correspond with the three right hand arcs
(from Environmental impacts to screening to EIA to satisfiction) in
Figure @ref(fig:netex).</p>
<hr>
<div class="figure">
<img src="netex/Slide1.png" alt="An annotated reproduction of Figure 1a from @varis97" width="110%" />
<p class="caption">
An annotated reproduction of Figure 1a from <span class="citation">Varis
(1997)</span>
</p>
</div>
<hr>
</div>
<div id="highlights-and-select-chapters-from-smith10"
class="section level2">
<h2>Highlights and select chapters from <span class="citation">Smith
(2010)</span></h2>
<p>The book from <span class="citation">Smith (2010)</span> about
Bayesian decision analysis is as applied as it gets.</p>
<hr>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berger85" class="csl-entry">
Berger, James O. 1985. <em>Statistical Decision Theory and Bayesian
Analysis</em>. 2nd ed. Springer Series in Statistics. New York:
Springer-Verlag.
</div>
<div id="ref-dorazio03" class="csl-entry">
Dorazio, Robert M., and Fred A. Johnson. 2003. <span>“BAYESIAN INFERENCE
AND DECISION THEORY—a FRAMEWORK FOR DECISION MAKING IN NATURAL RESOURCE
MANAGEMENT.”</span> <em>Ecological Applications</em> 13 (2): 556–63.
https://doi.org/<a
href="https://doi.org/10.1890/1051-0761(2003)013[0556:BIADTA]2.0.CO;2">https://doi.org/10.1890/1051-0761(2003)013[0556:BIADTA]2.0.CO;2</a>.
</div>
<div id="ref-gelman13" class="csl-entry">
Gelman, Andrew. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. Chapman
&amp; Hall/CRC Texts in Statistical Science. Boca Raton, FL: Chapman;
Hall/CRC, an imprint of Taylor; Francis.
</div>
<div id="ref-smith10" class="csl-entry">
Smith, J. Q. 2010. <em>Bayesian Decision Analysis : Principles and
Practice</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-varis97" class="csl-entry">
Varis, Olli. 1997. <span>“Bayesian Decision Analysis for Environmental
and Resource Management.”</span> <em>Environmental Modelling &amp;
Software</em> 12 (2): 177–85. https://doi.org/<a
href="https://doi.org/10.1016/S1364-8152(97)00008-X">https://doi.org/10.1016/S1364-8152(97)00008-X</a>.
</div>
<div id="ref-varis99" class="csl-entry">
Varis, Olli, and Sakari Kuikka. 1999. <span>“Learning Bayesian Decision
Analysis by Doing: Lessons from Environmental and Natural Resources
Management.”</span> <em>Ecological Modelling</em> 119 (2): 177–95.
https://doi.org/<a
href="https://doi.org/10.1016/S0304-3800(99)00061-7">https://doi.org/10.1016/S0304-3800(99)00061-7</a>.
</div>
<div id="ref-wade00" class="csl-entry">
Wade, Paul R. 2000. <span>“Bayesian Methods in Conservation
Biology.”</span> <em>Conservation Biology</em> 14 (5): 1308–16.
https://doi.org/<a
href="https://doi.org/10.1046/j.1523-1739.2000.99415.x">https://doi.org/10.1046/j.1523-1739.2000.99415.x</a>.
</div>
<div id="ref-williams16" class="csl-entry">
Williams, Perry J., and Mevin B. Hooten. 2016. <span>“Combining
Statistical Inference and Decisions in Ecology.”</span> <em>Ecological
Applications</em> 26 (6): 1930–42. https://doi.org/<a
href="https://doi.org/10.1890/15-1593.1">https://doi.org/10.1890/15-1593.1</a>.
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
